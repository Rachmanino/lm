from transformers.cache_utils import DynamicCache
import torch
from typing import Optional, Tuple, Any, Dict

class H2OCache(DynamicCache):
    def __init__(self, heavy_ratio = 0.1, recent_ratio = 0.1, num_hidden_layers = None):
        super().__init__(num_hidden_layers)
        self.heavy_ratio = heavy_ratio
        self.recent_ratio = recent_ratio

    def update(
        self,
        key_states: torch.Tensor,
        value_states: torch.Tensor,
        layer_idx: int,
        cache_kwargs: Optional[Dict[str, Any]] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.

        Parameters:
            key_states (`torch.Tensor`):
                The new key states to cache.
            value_states (`torch.Tensor`):
                The new value states to cache.
            layer_idx (`int`):
                The index of the layer to cache the states for.
            cache_kwargs (`Dict[str, Any]`, `optional`):
                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.

        Return:
            A tuple containing the updated key and value states.
        """
        # Update the number of seen tokens
        if layer_idx == 0:
            self._seen_tokens += key_states.shape[-2]
